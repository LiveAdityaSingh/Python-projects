{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LiveAdityaSingh/Python-projects/blob/main/Text%20synthesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0RIb-AnDyQi"
      },
      "source": [
        "#P3 Automatic Question Generation\n",
        "text=\"Artificial intelligence refers to the simulation of human intelligence in machines that are programmed to think humans and mimic their actions. The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving.\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asGI74GhFg4f"
      },
      "source": [
        "# Common imports\n",
        "import pandas as pd\n",
        "from IPython.display import Markdown, display, clear_output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGdcNsOfKggR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f9d35e1-7633-403a-8b78-7ef576549d1d"
      },
      "source": [
        "import _pickle as cPickle\n",
        "from pathlib import Path\n",
        "\n",
        "def dumpPickle(fileName, content):\n",
        "    pickleFile = open(fileName, 'wb')\n",
        "    cPickle.dump(content, pickleFile, -1)\n",
        "    pickleFile.close()\n",
        "\n",
        "def loadPickle(fileName):\n",
        "\n",
        "    file = open(fileName, 'rb')\n",
        "    content = cPickle.load(file)\n",
        "    file.close()\n",
        "\n",
        "    return content\n",
        "\n",
        "def pickleExists(fileName):\n",
        "    file = Path(fileName)\n",
        "\n",
        "    if file.is_file():\n",
        "        loadPickle(fileName)\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "print(\"Proceed to next step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Proceed to next step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57tVrP4cKuFd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f82dfd66-4205-444e-d818-30f4f60c6a19"
      },
      "source": [
        "#Data Exploration\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "#Extract answers and the sentence they are in\n",
        "def extractAnswers(qas, doc):\n",
        "    answers = []\n",
        "\n",
        "    senStart = 0\n",
        "    senId = 0\n",
        "\n",
        "    for sentence in doc.sents:\n",
        "        senLen = len(sentence.text)\n",
        "\n",
        "        for answer in qas:\n",
        "            answerStart = answer['answers'][0]['answer_start']\n",
        "\n",
        "            if (answerStart >= senStart and answerStart < (senStart + senLen)):\n",
        "                answers.append({'sentenceId': senId, 'text': answer['answers'][0]['text']})\n",
        "\n",
        "        senStart += senLen\n",
        "        senId += 1\n",
        "\n",
        "    return answers\n",
        "\n",
        "#TODO - Clean answers from stopwords?\n",
        "def tokenIsAnswer(token, sentenceId, answers):\n",
        "    for i in range(len(answers)):\n",
        "        if (answers[i]['sentenceId'] == sentenceId):\n",
        "            if (answers[i]['text'] == token):\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "#Save named entities start points\n",
        "\n",
        "def getNEStartIndexs(doc):\n",
        "    neStarts = {}\n",
        "    for ne in doc.ents:\n",
        "        neStarts[ne.start] = ne\n",
        "\n",
        "    return neStarts\n",
        "\n",
        "def getSentenceStartIndexes(doc):\n",
        "    senStarts = []\n",
        "\n",
        "    for sentence in doc.sents:\n",
        "        senStarts.append(sentence[0].i)\n",
        "\n",
        "    return senStarts\n",
        "\n",
        "def getSentenceForWordPosition(wordPos, senStarts):\n",
        "    for i in range(1, len(senStarts)):\n",
        "        if (wordPos < senStarts[i]):\n",
        "            return i - 1\n",
        "#Raw text to array\n",
        "def addWordsForParagrapgh(newWords, text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    neStarts = getNEStartIndexs(doc)\n",
        "    senStarts = getSentenceStartIndexes(doc)\n",
        "\n",
        "    #index of word in spacy doc text\n",
        "    i = 0\n",
        "\n",
        "    while (i < len(doc)):\n",
        "        #If the token is a start of a Named Entity, add it and push to index to end of the NE\n",
        "        if (i in neStarts):\n",
        "            word = neStarts[i]\n",
        "            #add word\n",
        "            currentSentence = getSentenceForWordPosition(word.start, senStarts)\n",
        "            wordLen = word.end - word.start\n",
        "            shape = ''\n",
        "            for wordIndex in range(word.start, word.end):\n",
        "                shape += (' ' + doc[wordIndex].shape_)\n",
        "\n",
        "            newWords.append([word.text,\n",
        "                            0,\n",
        "                            0,\n",
        "                            currentSentence,\n",
        "                            wordLen,\n",
        "                            word.label_,\n",
        "                            None,\n",
        "                            None,\n",
        "                            None,\n",
        "                            shape])\n",
        "            i = neStarts[i].end - 1\n",
        "        #If not a NE, add the word if it's not a stopword or a non-alpha (not regular letters)\n",
        "        else:\n",
        "            if (doc[i].is_stop == False and doc[i].is_alpha == True):\n",
        "                word = doc[i]\n",
        "\n",
        "                currentSentence = getSentenceForWordPosition(i, senStarts)\n",
        "                wordLen = 1\n",
        "\n",
        "                newWords.append([word.text,\n",
        "                                0,\n",
        "                                0,\n",
        "                                currentSentence,\n",
        "                                wordLen,\n",
        "                                None,\n",
        "                                word.pos_,\n",
        "                                word.tag_,\n",
        "                                word.dep_,\n",
        "                                word.shape_])\n",
        "        i += 1\n",
        "\n",
        "def oneHotEncodeColumns(df):\n",
        "    columnsToEncode = ['NER', 'POS', \"TAG\", 'DEP']\n",
        "\n",
        "    for column in columnsToEncode:\n",
        "        one_hot = pd.get_dummies(df[column])\n",
        "        one_hot = one_hot.add_prefix(column + '_')\n",
        "\n",
        "        df = df.drop(column, axis = 1)\n",
        "        df = df.join(one_hot)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "print(\"proceed to next step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proceed to next step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKR2C0pWKyJ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3042cb4c-6ff2-45ea-f1bb-e35a63a5119e"
      },
      "source": [
        "def generateDf(text):\n",
        "    words = []\n",
        "    addWordsForParagrapgh(words, text)\n",
        "\n",
        "    wordColums = ['text', 'titleId', 'paragrapghId', 'sentenceId','wordCount', 'NER', 'POS', 'TAG', 'DEP','shape']\n",
        "    df = pd.DataFrame(words, columns=wordColums)\n",
        "\n",
        "    return df\n",
        "print(\"proceed to next step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proceed to next step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlAa0AdIKzUM"
      },
      "source": [
        "def prepareDf(df):\n",
        "    #One-hot encoding\n",
        "    wordsDf = oneHotEncodeColumns(df)\n",
        "\n",
        "    #Drop un-usefull columns\n",
        "    columnsToDrop = ['text', 'titleId', 'paragrapghId', 'sentenceId','shape' ]\n",
        "    wordsDf = wordsDf.drop(columnsToDrop, axis = 1)\n",
        "\n",
        "    #Add token colums\n",
        "    predictorColumns = ['wordCount','NER_CARDINAL','NER_DATE','NER_EVENT','NER_FAC','NER_GPE','NER_LANGUAGE',\n",
        "                        'NER_LAW','NER_LOC','NER_MONEY','NER_NORP','NER_ORDINAL','NER_ORG','NER_PERCENT',\n",
        "                        'NER_PERSON','NER_PRODUCT','NER_QUANTITY','NER_TIME','NER_WORK_OF_ART','POS_ADJ',\n",
        "                        'POS_ADP','POS_ADV','POS_CCONJ','POS_DET','POS_INTJ','POS_NOUN','POS_NUM','POS_PART',\n",
        "                        'POS_PRON','POS_PROPN','POS_PUNCT','POS_SYM','POS_VERB','POS_X','TAG_''','TAG_-LRB-',\n",
        "                        'TAG_.','TAG_ADD','TAG_AFX','TAG_CC','TAG_CD','TAG_DT','TAG_EX','TAG_FW','TAG_IN',\n",
        "                        'TAG_JJ','TAG_JJR','TAG_JJS','TAG_LS','TAG_MD','TAG_NFP','TAG_NN','TAG_NNP','TAG_NNPS',\n",
        "                        'TAG_NNS','TAG_PDT','TAG_POS','TAG_PRP','TAG_PRP$','TAG_RB','TAG_RBR','TAG_RBS',\n",
        "                        'TAG_RP','TAG_SYM','TAG_TO','TAG_UH','TAG_VB','TAG_VBD','TAG_VBG','TAG_VBN','TAG_VBP',\n",
        "                        'TAG_VBZ','TAG_WDT','TAG_WP','TAG_WRB','TAG_XX','DEP_ROOT','DEP_acl','DEP_acomp',\n",
        "                        'DEP_advcl','DEP_advmod','DEP_agent','DEP_amod','DEP_appos','DEP_attr','DEP_aux',\n",
        "                        'DEP_auxpass','DEP_case','DEP_cc','DEP_ccomp','DEP_compound','DEP_conj','DEP_csubj',\n",
        "                        'DEP_csubjpass','DEP_dative','DEP_dep','DEP_det','DEP_dobj','DEP_expl','DEP_intj',\n",
        "                        'DEP_mark','DEP_meta','DEP_neg','DEP_nmod','DEP_npadvmod','DEP_nsubj','DEP_nsubjpass',\n",
        "                        'DEP_nummod','DEP_oprd','DEP_parataxis','DEP_pcomp','DEP_pobj','DEP_poss','DEP_preconj',\n",
        "                        'DEP_predet','DEP_prep','DEP_prt','DEP_punct','DEP_quantmod','DEP_relcl','DEP_xcomp']\n",
        "\n",
        "    for feature in predictorColumns:\n",
        "        if feature not in wordsDf.columns:\n",
        "            wordsDf[feature] = 0\n",
        "\n",
        "    #for column in predictorColumns:\n",
        "     #   if (column in df.columns):\n",
        "      #      wordsDf[column] = df[column]\n",
        "       # else:\n",
        "        #    wordsDf[column] = 0\n",
        "    print(\"proceed to next step\")\n",
        "\n",
        "    return wordsDf\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZKxaaA8K3ZS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df413768-0bf7-4f60-82c9-d8901467204b"
      },
      "source": [
        "def predictWords(wordsDf, df):\n",
        "\n",
        "    predictorPickleName = 'data/pickles/nb-predictor.pkl'\n",
        "    predictor = loadPickle(predictorPickleName)\n",
        "\n",
        "    y_pred = predictor.predict_proba(wordsDf)\n",
        "\n",
        "    labeledAnswers = []\n",
        "    for i in range(len(y_pred)):\n",
        "        labeledAnswers.append({'word': df.iloc[i]['text'], 'prob': y_pred[i][0]})\n",
        "\n",
        "    return labeledAnswers\n",
        "\n",
        "print(\"proceed to next step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proceed to next step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGlf26QjK6yL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8398e88-ddac-4df3-ddfa-a821156bba4a"
      },
      "source": [
        "def blankAnswer(firstTokenIndex, lastTokenIndex, sentStart, sentEnd, doc):\n",
        "    leftPartStart = doc[sentStart].idx\n",
        "    leftPartEnd = doc[firstTokenIndex].idx\n",
        "    rightPartStart = doc[lastTokenIndex].idx + len(doc[lastTokenIndex])\n",
        "    rightPartEnd = doc[sentEnd - 1].idx + len(doc[sentEnd - 1])\n",
        "\n",
        "    question = doc.text[leftPartStart:leftPartEnd] + '_____' + doc.text[rightPartStart:rightPartEnd]\n",
        "\n",
        "    return question\n",
        "\n",
        "print(\"proceed to next step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proceed to next step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5-eaw58K9or",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a412e0b2-77b1-47d4-8f6c-fc0a2ebe375e"
      },
      "source": [
        "#token to sentences\n",
        "\n",
        "def addQuestions(answers, text):\n",
        "    doc = nlp(text)\n",
        "    currAnswerIndex = 0\n",
        "    qaPair = []\n",
        "\n",
        "    #Check wheter each token is the next answer\n",
        "    for sent in doc.sents:\n",
        "        for token in sent:\n",
        "\n",
        "            #If all the answers have been found, stop looking\n",
        "            if currAnswerIndex >= len(answers):\n",
        "                break\n",
        "\n",
        "            #In the case where the answer is consisted of more than one token, check the following tokens as well.\n",
        "            answerDoc = nlp(answers[currAnswerIndex]['word'])\n",
        "            answerIsFound = True\n",
        "\n",
        "            for j in range(len(answerDoc)):\n",
        "                if token.i + j >= len(doc) or doc[token.i + j].text != answerDoc[j].text:\n",
        "                    answerIsFound = False\n",
        "\n",
        "            #If the current token is corresponding with the answer, add it\n",
        "            if answerIsFound:\n",
        "                question = blankAnswer(token.i, token.i + len(answerDoc) - 1, sent.start, sent.end, doc)\n",
        "\n",
        "                qaPair.append({'question' : question, 'answer': answers[currAnswerIndex]['word'], 'prob': answers[currAnswerIndex]['prob']})\n",
        "\n",
        "                currAnswerIndex += 1\n",
        "\n",
        "    return qaPair\n",
        "\n",
        "print(\"proceed to next step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proceed to next step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14H5p-G4LAyF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcf3090d-84f1-4d57-9e76-1685cf4bbd29"
      },
      "source": [
        "#Follow-up\n",
        "def sortAnswers(qaPairs):\n",
        "    orderedQaPairs = sorted(qaPairs, key=lambda qaPair: qaPair['prob'])\n",
        "\n",
        "    return orderedQaPairs\n",
        "\n",
        "print(\"proceed to next step\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proceed to next step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhhIpNG8LGP3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "58bc3f4c-6ad7-4907-ba79-d2ba57dd8e72"
      },
      "source": [
        "print(\"Wrong answers ready\")\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "glove_file = 'data/embeddings/glove.6B.300d.txt'\n",
        "tmp_file = 'data/embeddings/word2vec-glove.6B.300d.txt'\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove2word2vec(glove_file, tmp_file)\n",
        "model = KeyedVectors.load_word2vec_format(tmp_file,binary=True)\n",
        "\n",
        "\n",
        "def generate_distractors(answer, count):\n",
        "    answer = str.lower(answer)\n",
        "\n",
        "    ##Extracting closest words for the answer.\n",
        "    try:\n",
        "        closestWords = model.most_similar(positive=[answer], topn=count)\n",
        "    except:\n",
        "        #In case the word is not in the vocabulary, or other problem not loading embeddings\n",
        "        return []\n",
        "\n",
        "    #Return count many distractors\n",
        "    distractors = list(map(lambda x: x[0], closestWords))[0:count]\n",
        "\n",
        "    return distractors\n",
        "def addDistractors(qaPairs, count):\n",
        "    for qaPair in qaPairs:\n",
        "        distractors = generate_distractors(qaPair['answer'], count)\n",
        "        qaPair['distractors'] = distractors\n",
        "\n",
        "    return qaPairs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrong answers ready\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nI5oX6RLNIN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e11d300-ae84-47d6-aa3d-245f8444d4ba"
      },
      "source": [
        "#Wrap-up and display\n",
        "def generateQuestions(text, count):\n",
        "\n",
        "    # Extract words\n",
        "    df = generateDf(text)\n",
        "    wordsDf = prepareDf(df)\n",
        "\n",
        "    # Predict\n",
        "    labeledAnswers = predictWords(wordsDf, df)\n",
        "\n",
        "    # Transform questions\n",
        "    qaPairs = addQuestions(labeledAnswers, text)\n",
        "\n",
        "    # Pick the best questions\n",
        "    orderedQaPairs = sortAnswers(qaPairs)\n",
        "\n",
        "    # Generate distractors\n",
        "    questions = addDistractors(orderedQaPairs[:count], 3)\n",
        "\n",
        "    # Print\n",
        "    for i in range(count):\n",
        "        display(Markdown('### Question ' + str(i + 1) + ':'))\n",
        "        print(questions[i]['question'])\n",
        "\n",
        "        display(Markdown('#### Options:'))\n",
        "        print(questions[i]['answer'])\n",
        "        for distractor in questions[i]['distractors']:\n",
        "            print(distractor)\n",
        "\n",
        "        print()\n",
        "\n",
        "print(\"Display Ready\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Display Ready\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q06A1Y2LQaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "962bad0c-8fbb-4619-dd1c-ada5c272fa0a"
      },
      "source": [
        "#with open (\"data.txt\", \"r\") as myfile:\n",
        " #               text=myfile.read()\n",
        "#print(\"Test Data: \",text)\n",
        "generateQuestions(text, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proceed to next step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator GaussianNB from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:1366: RuntimeWarning: overflow encountered in square\n",
            "  self.vectors_norm = (self.vectors / sqrt((self.vectors ** 2).sum(-1))[..., newaxis]).astype(REAL)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Question 1:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "The term may also be applied to any machine that exhibits traits _____ with a human mind such as learning and problem-solving.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Options:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "associated\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Question 2:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "The term may also be _____ to any machine that exhibits traits associated with a human mind such as learning and problem-solving.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Options:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "applied\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Question 3:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Artificial intelligence refers to the simulation of human intelligence in machines that are programmed to think _____ and mimic their actions.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Options:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "humans\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Question 4:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Artificial intelligence refers to the simulation of human intelligence in machines that are programmed to think humans and mimic their _____.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Options:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "actions\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Question 5:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "The term may also be applied to any machine that exhibits _____ associated with a human mind such as learning and problem-solving.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Options:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "traits\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Question 6:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Artificial intelligence refers to the simulation of human intelligence in machines that are programmed to think humans and _____ their actions.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Options:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "mimic\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Question 7:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Artificial _____ refers to the simulation of human intelligence in machines that are programmed to think humans and mimic their actions.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Options:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "intelligence\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Question 8:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "_____ intelligence refers to the simulation of human intelligence in machines that are programmed to think humans and mimic their actions.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Options:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Artificial\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Question 9:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Artificial intelligence refers to the simulation of _____ intelligence in machines that are programmed to think humans and mimic their actions.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Options:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "human\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Question 10:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "The term may also be applied to any machine that exhibits traits associated with a _____ mind such as learning and problem-solving.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "#### Options:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "human\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}